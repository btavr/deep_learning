{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "5942b511",
      "metadata": {},
      "source": [
        "# In-class Worksheet ‚Äî scikit-learn Intro (Binary Classification)\n",
        "\n",
        "**Dataset:** Wisconsin Breast Cancer (Diagnostic)  \n",
        "**Goal:** Follow along as the lesson progresses. Fill in each **TODO** section.\n",
        "\n",
        "## Rules (for this worksheet)\n",
        "- This is **not** a quiz. Use your notes and discuss with peers.\n",
        "- Keep `random_state=42` where specified so results are reproducible.\n",
        "- Do **not** rename variables that the worksheet defines (the grader will look for them).\n",
        "\n",
        "## Grading\n",
        "At the end, you will run a correction cell that prints a **score / 100** plus feedback.  \n",
        "The score is only to help you self-check completion; it is not an exam.\n",
        "\n",
        "---\n",
        "\n",
        "## üîé Why Exact Variable Names Matter\n",
        "\n",
        "This worksheet includes an automatic self-check grading script.\n",
        "\n",
        "The grader:\n",
        "- Checks specific variable names (e.g., `X_train`, `pipe`, `test_accuracy`)\n",
        "- Checks specific Pipeline step names (`\"scaler\"` and `\"clf\"`)\n",
        "- Verifies expected structures and shapes\n",
        "\n",
        "Even if your code is logically correct, using different variable or step names\n",
        "(e.g., `\"scale\"` instead of `\"scaler\"`) may cause the grader to mark it as incorrect.\n",
        "\n",
        "This is done only to:\n",
        "- Keep grading consistent\n",
        "- Avoid ambiguity\n",
        "- Ensure reproducibility\n",
        "\n",
        "In real projects, step names are flexible ‚Äî here they must match exactly.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9a73caf2",
      "metadata": {},
      "source": [
        "## 0) Setup\n",
        "\n",
        "Run the cell below to import libraries.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "084b2723",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Setup complete.\n"
          ]
        }
      ],
      "source": [
        "# Imports (run once)\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split, ShuffleSplit, cross_val_score\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "from sklearn.metrics import (\n",
        "    confusion_matrix, accuracy_score,\n",
        "    precision_score, recall_score, f1_score,\n",
        "    roc_curve, auc\n",
        ")\n",
        "\n",
        "print(\"Setup complete.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ca3939fa",
      "metadata": {},
      "source": [
        "## 1) Load the dataset\n",
        "\n",
        "**Tasks**\n",
        "1. Load the breast cancer dataset using `load_breast_cancer`.\n",
        "2. Store:\n",
        "   - features in `X`\n",
        "   - labels in `y`\n",
        "3. Print:\n",
        "   - `X.shape`\n",
        "   - class distribution (counts of 0 and 1)\n",
        "\n",
        "**Notes**\n",
        "- This is a **binary** classification dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "bb3d8d64",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "X.shape: (569, 30)\n",
            "Class distribution (0 and 1): {0: 212, 1: 357}\n"
          ]
        }
      ],
      "source": [
        "# TODO 1: Load dataset\n",
        "# - X, y\n",
        "# - print X.shape\n",
        "# - print class counts (0 and 1)\n",
        "\n",
        "# YOUR CODE HERE\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "print(\"X.shape:\", X.shape)\n",
        "print(\"Class distribution (0 and 1):\", dict(zip(*np.unique(y, return_counts=True))))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "721ae655",
      "metadata": {},
      "source": [
        "## 2) Train/test split (stratified)\n",
        "\n",
        "‚ö†Ô∏è **IMPORTANT ‚Äî Use the exact variable names below (required for grading):**\n",
        "\n",
        "You must create:\n",
        "\n",
        "- `X_train`\n",
        "- `X_test`\n",
        "- `y_train`\n",
        "- `y_test`\n",
        "\n",
        "**Tasks**\n",
        "1. Split into train/test with:\n",
        "   - `test_size=0.2`\n",
        "   - `random_state=42`\n",
        "   - `stratify=y`\n",
        "2. Store results exactly in:\n",
        "   - `X_train, X_test, y_train, y_test`\n",
        "3. Print class proportions in train and test to confirm stratification.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "86b3e5e5",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train class proportions (0, 1): [0.37362637 0.62637363]\n",
            "Test class proportions (0, 1): [0.36842105 0.63157895]\n"
          ]
        }
      ],
      "source": [
        "# TODO 2: Stratified split\n",
        "# - X_train, X_test, y_train, y_test\n",
        "# - print class proportions in train/test\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "_, counts_train = np.unique(y_train, return_counts=True)\n",
        "_, counts_test = np.unique(y_test, return_counts=True)\n",
        "print(\"Train class proportions (0, 1):\", counts_train / counts_train.sum())\n",
        "print(\"Test class proportions (0, 1):\", counts_test / counts_test.sum())\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5320aa8f",
      "metadata": {},
      "source": [
        "## 3) Build a Pipeline and train a classifier\n",
        "\n",
        "We will use:\n",
        "\n",
        "- `StandardScaler()`\n",
        "- `LogisticRegression(max_iter=2000)`\n",
        "\n",
        "‚ö†Ô∏è **IMPORTANT ‚Äî Use the exact names below (required for grading):**\n",
        "\n",
        "Create a Pipeline named **`pipe`** with the following step names:\n",
        "\n",
        "```python\n",
        "pipe = Pipeline([\n",
        "    (\"scaler\", StandardScaler()),\n",
        "    (\"clf\", LogisticRegression(max_iter=2000))\n",
        "])\n",
        "```\n",
        "\n",
        "**Tasks**\n",
        "1. Build the `pipe` exactly as shown above (same step names).\n",
        "2. Fit it on the training data.\n",
        "3. Compute:\n",
        "   - `test_accuracy` using `pipe.score(X_test, y_test)`\n",
        "   - `y_pred` using `pipe.predict(X_test)`\n",
        "\n",
        "Print `test_accuracy`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "2aa472ea",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test accuracy: 0.9824561403508771\n"
          ]
        }
      ],
      "source": [
        "# TODO 3: Pipeline + training\n",
        "# - pipe\n",
        "# - fit\n",
        "# - test_accuracy\n",
        "# - y_pred\n",
        "\n",
        "# YOUR CODE HERE\n",
        "pipe = Pipeline([\n",
        "    (\"scaler\", StandardScaler()),\n",
        "    (\"clf\", LogisticRegression(max_iter=2000, random_state=42))\n",
        "])\n",
        "pipe.fit(X_train, y_train)\n",
        "test_accuracy = pipe.score(X_test, y_test)\n",
        "y_pred = pipe.predict(X_test)\n",
        "print(\"Test accuracy:\", test_accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dab788dc",
      "metadata": {},
      "source": [
        "## 4) Evaluate with confusion matrix + metrics\n",
        "\n",
        "**Tasks**\n",
        "1. Compute the confusion matrix and store it in `cm`.\n",
        "2. Compute and store the following floats:\n",
        "   - `test_precision`\n",
        "   - `test_recall`\n",
        "   - `test_f1`\n",
        "\n",
        "Print the confusion matrix and the metrics.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "b8d19e1d",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Confusion Matrix:\n",
            " [[41  1]\n",
            " [ 1 71]]\n",
            "Test Precision: 0.9861111111111112\n",
            "Test Recall: 0.9861111111111112\n",
            "Test F1 Score: 0.9861111111111112\n"
          ]
        }
      ],
      "source": [
        "# TODO 4: Confusion matrix + metrics\n",
        "# - cm, test_precision, test_recall, test_f1\n",
        "\n",
        "# YOUR CODE HERE\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "test_precision = precision_score(y_test, y_pred)\n",
        "test_recall = recall_score(y_test, y_pred)\n",
        "test_f1 = f1_score(y_test, y_pred)\n",
        "print(\"Confusion Matrix:\\n\", cm)\n",
        "print(\"Test Precision:\", test_precision)\n",
        "print(\"Test Recall:\", test_recall)\n",
        "print(\"Test F1 Score:\", test_f1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "14e08e0c",
      "metadata": {},
      "source": [
        "## 5) Adjust the decision threshold (binary)\n",
        "\n",
        "In medical screening, **recall** is often important.\n",
        "\n",
        "‚ö†Ô∏è **IMPORTANT ‚Äî Use the exact variable names below (required for grading):**\n",
        "\n",
        "You must create:\n",
        "\n",
        "- `probs_pos`\n",
        "- `tau` (must be exactly `0.30`)\n",
        "- `y_pred_tau`\n",
        "- `recall_tau`\n",
        "- `cm_tau`\n",
        "\n",
        "**Tasks**\n",
        "1. Compute predicted probabilities for the positive class:\n",
        "   - store in `probs_pos`\n",
        "2. Set `tau = 0.30`\n",
        "3. Create predictions with this threshold:\n",
        "   - store in `y_pred_tau`\n",
        "4. Compute and store:\n",
        "   - `recall_tau`\n",
        "   - `cm_tau`\n",
        "\n",
        "Print `recall_tau` and `cm_tau`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "0d3df6c8",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Recall tau: 1.0\n",
            "Confusion Matrix tau:\n",
            " [[40  2]\n",
            " [ 0 72]]\n"
          ]
        }
      ],
      "source": [
        "# TODO 5: Threshold adjustment\n",
        "# - probs_pos\n",
        "# - tau = 0.30\n",
        "# - y_pred_tau\n",
        "# - recall_tau\n",
        "# - cm_tau\n",
        "\n",
        "# YOUR CODE HERE\n",
        "probs_pos = pipe.predict_proba(X_test)[:, 1]\n",
        "tau = 0.30\n",
        "y_pred_tau = (probs_pos >= tau).astype(int)\n",
        "recall_tau = recall_score(y_test, y_pred_tau)\n",
        "cm_tau = confusion_matrix(y_test, y_pred_tau)\n",
        "print(\"Recall tau:\", recall_tau)\n",
        "print(\"Confusion Matrix tau:\\n\", cm_tau)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "410150e2",
      "metadata": {},
      "source": [
        "## 6) ROC curve + AUC (binary)\n",
        "\n",
        "**Tasks**\n",
        "1. Using `probs_pos`, compute:\n",
        "   - `fpr`, `tpr`, `_` using `roc_curve`\n",
        "2. Compute:\n",
        "   - `roc_auc` using `auc(fpr, tpr)`\n",
        "\n",
        "Print `roc_auc`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "0110a3d1",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ROC AUC: 0.9953703703703703\n"
          ]
        }
      ],
      "source": [
        "# TODO 6: ROC + AUC\n",
        "# - fpr, tpr, roc_auc\n",
        "\n",
        "# YOUR CODE HERE\n",
        "fpr, tpr, _ = roc_curve(y_test, probs_pos)\n",
        "roc_auc = auc(fpr, tpr)\n",
        "print(\"ROC AUC:\", roc_auc)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7c208eec",
      "metadata": {},
      "source": [
        "## 7) Cross-validation with ShuffleSplit\n",
        "\n",
        "**Tasks**\n",
        "1. Create a `ShuffleSplit` object named `cv_ss` with:\n",
        "   - `n_splits=20`\n",
        "   - `test_size=0.2`\n",
        "   - `random_state=42`\n",
        "2. Compute cross-validation accuracy scores with:\n",
        "   - `cross_val_score(pipe, X, y, cv=cv_ss, scoring=\"accuracy\")`\n",
        "   - store the array in `cv_scores`\n",
        "3. Store:\n",
        "   - `cv_mean` (mean of `cv_scores`)\n",
        "   - `cv_std` (std of `cv_scores`)\n",
        "\n",
        "Print `cv_mean` and `cv_std`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "8ca98aca",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CV mean: 0.9811403508771928\n",
            "CV std: 0.012178459102556851\n"
          ]
        }
      ],
      "source": [
        "# TODO 7: Cross-validation with ShuffleSplit\n",
        "# - cv_ss, cv_scores, cv_mean, cv_std\n",
        "\n",
        "# YOUR CODE HERE\n",
        "cv_ss = ShuffleSplit(n_splits= 20, test_size=0.2, random_state=42)\n",
        "cv_scores = cross_val_score(pipe, X, y, cv=cv_ss, scoring=\"accuracy\")\n",
        "cv_mean = cv_scores.mean()\n",
        "cv_std = cv_scores.std()\n",
        "print(\"CV mean:\", cv_mean)\n",
        "print(\"CV std:\", cv_std)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8ba8cf2e",
      "metadata": {},
      "source": [
        "## 8) Hyperparameter search: GridSearchCV\n",
        "\n",
        "‚ö†Ô∏è **IMPORTANT ‚Äî Use the exact names below (required for grading):**\n",
        "\n",
        "You must create:\n",
        "\n",
        "- `param_grid`\n",
        "- `grid`\n",
        "- `best_params`\n",
        "- `best_f1`\n",
        "\n",
        "Parameter grid must be:\n",
        "\n",
        "```python\n",
        "param_grid = {\n",
        "    \"clf__C\": [0.01, 0.1, 1.0, 10.0, 100.0]\n",
        "}\n",
        "```\n",
        "\n",
        "`GridSearchCV` must use:\n",
        "\n",
        "- `estimator=pipe`\n",
        "- `cv=5`\n",
        "- `scoring=\"f1\"`\n",
        "- `n_jobs=-1`\n",
        "\n",
        "After fitting, store:\n",
        "\n",
        "```python\n",
        "best_params = grid.best_params_\n",
        "best_f1 = grid.best_score_\n",
        "```\n",
        "\n",
        "Print `best_params` and `best_f1`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "e1902228",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best params: {'clf__C': 0.1}\n",
            "Best score: 0.984524686809138\n"
          ]
        }
      ],
      "source": [
        "# TODO 8: GridSearchCV\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# - param_grid\n",
        "# - grid\n",
        "# - best_params, best_f1\n",
        "\n",
        "# YOUR CODE HERE\n",
        "param_grid = {\n",
        "    \"clf__C\": [0.01, 0.1, 1.0, 10.0, 100.0]\n",
        "}\n",
        "\n",
        "grid = GridSearchCV(\n",
        "    estimator=pipe,\n",
        "    param_grid=param_grid,\n",
        "    cv=5,\n",
        "    scoring=\"f1\",\n",
        "    n_jobs=-1\n",
        ")\n",
        "grid.fit(X_train, y_train)\n",
        "\n",
        "best_params = grid.best_params_\n",
        "best_f1 = grid.best_score_\n",
        "\n",
        "print(\"Best params:\", best_params)\n",
        "print(\"Best score:\", best_f1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1ee15123",
      "metadata": {},
      "source": [
        "## 9) Correction / self-check (run at the end)\n",
        "\n",
        "Run the cell below **after you completed all TODOs**.\n",
        "It will output a score and feedback.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0347ddad",
      "metadata": {},
      "outputs": [
        {
          "ename": "SyntaxError",
          "evalue": "invalid decimal literal (731910549.py, line 2)",
          "output_type": "error",
          "traceback": [
            "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mfrom 01_grader_breast_cancer import grade\u001b[39m\n           ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid decimal literal\n"
          ]
        }
      ],
      "source": [
        "# Run correction (self-check)\n",
        "from grader_breast_cancer import grade\n",
        "\n",
        "result = grade(globals())\n",
        "result\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "dl-venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
